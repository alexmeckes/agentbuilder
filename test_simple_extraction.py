#!/usr/bin/env python3
"""
Simple test to verify the trace extraction fixes work
"""

def test_extraction_logic():
    """Test the basic extraction logic without dependencies"""
    
    print("ðŸ§ª Testing attribute extraction logic...")
    
    # Mock span data that mimics what any-agent provides
    mock_spans = [
        {
            "attributes": {
                # OpenInference convention (what any-agent actually uses)
                "llm.model_name": "gpt-4o-mini",
                "llm.token_count.prompt": 150,
                "llm.token_count.completion": 75,
                "cost_prompt": 0.00015,
                "cost_completion": 0.000225,
                "openinference.span.kind": "LLM"
            }
        },
        {
            "attributes": {
                "tool.name": "search_web",
                "input.value": "test query",
                "output.value": "search results",
                "openinference.span.kind": "TOOL"
            }
        }
    ]
    
    # Test the extraction logic from our fixes
    total_cost = 0.0
    total_tokens = 0
    input_tokens = 0
    output_tokens = 0
    
    for i, span in enumerate(mock_spans):
        attributes = span.get("attributes", {})
        
        print(f"Span {i} attributes: {list(attributes.keys())}")
        
        # Extract using our fixed logic (GenAI with OpenInference fallbacks)
        span_input_tokens = attributes.get("gen_ai.usage.input_tokens", attributes.get("llm.token_count.prompt", 0))
        span_output_tokens = attributes.get("gen_ai.usage.output_tokens", attributes.get("llm.token_count.completion", 0))
        span_input_cost = attributes.get("gen_ai.usage.input_cost", attributes.get("cost_prompt", 0.0))
        span_output_cost = attributes.get("gen_ai.usage.output_cost", attributes.get("cost_completion", 0.0))
        
        span_total_cost = float(span_input_cost) + float(span_output_cost)
        span_total_tokens = int(span_input_tokens) + int(span_output_tokens)
        
        print(f"  Extracted: tokens={span_input_tokens}+{span_output_tokens}={span_total_tokens}, cost=${span_input_cost:.6f}+${span_output_cost:.6f}=${span_total_cost:.6f}")
        
        total_cost += span_total_cost
        input_tokens += int(span_input_tokens)
        output_tokens += int(span_output_tokens)
        total_tokens += span_total_tokens
    
    print(f"\nðŸ“Š Final Results:")
    print(f"  Total Cost: ${total_cost:.6f}")
    print(f"  Total Tokens: {total_tokens}")
    print(f"  Input Tokens: {input_tokens}")
    print(f"  Output Tokens: {output_tokens}")
    
    # Verify results
    expected_cost = 0.000375  # 0.00015 + 0.000225
    expected_tokens = 225     # 150 + 75
    expected_input = 150
    expected_output = 75
    
    success = True
    
    if abs(total_cost - expected_cost) < 0.000001:
        print("âœ… Cost extraction PASSED")
    else:
        print(f"âŒ Cost extraction FAILED: got ${total_cost:.6f}, expected ${expected_cost:.6f}")
        success = False
        
    if total_tokens == expected_tokens:
        print("âœ… Token extraction PASSED")
    else:
        print(f"âŒ Token extraction FAILED: got {total_tokens}, expected {expected_tokens}")
        success = False
        
    if input_tokens == expected_input and output_tokens == expected_output:
        print("âœ… Token breakdown PASSED")
    else:
        print(f"âŒ Token breakdown FAILED: got {input_tokens}/{output_tokens}, expected {expected_input}/{expected_output}")
        success = False
    
    return success

def test_fallback_priority():
    """Test that GenAI attributes take precedence over OpenInference"""
    
    print("\nðŸ§ª Testing attribute fallback priority...")
    
    # Test case with both GenAI and OpenInference attributes
    attributes = {
        # GenAI (should take precedence)
        "gen_ai.usage.input_tokens": 200,
        "gen_ai.usage.output_tokens": 100,
        "gen_ai.usage.input_cost": 0.0002,
        "gen_ai.usage.output_cost": 0.0003,
        # OpenInference (should be ignored when GenAI is present)
        "llm.token_count.prompt": 50,
        "llm.token_count.completion": 25,
        "cost_prompt": 0.00005,
        "cost_completion": 0.00007
    }
    
    # Extract using our fixed logic
    input_tokens = attributes.get("gen_ai.usage.input_tokens", attributes.get("llm.token_count.prompt", 0))
    output_tokens = attributes.get("gen_ai.usage.output_tokens", attributes.get("llm.token_count.completion", 0))
    input_cost = attributes.get("gen_ai.usage.input_cost", attributes.get("cost_prompt", 0.0))
    output_cost = attributes.get("gen_ai.usage.output_cost", attributes.get("cost_completion", 0.0))
    
    print(f"Extracted: tokens={input_tokens}+{output_tokens}, cost=${input_cost:.6f}+${output_cost:.6f}")
    
    # Should use GenAI values, not OpenInference
    expected_input_tokens = 200
    expected_output_tokens = 100
    expected_input_cost = 0.0002
    expected_output_cost = 0.0003
    
    success = True
    
    if input_tokens == expected_input_tokens and output_tokens == expected_output_tokens:
        print("âœ… Token precedence PASSED")
    else:
        print(f"âŒ Token precedence FAILED: got {input_tokens}/{output_tokens}, expected {expected_input_tokens}/{expected_output_tokens}")
        success = False
        
    if abs(input_cost - expected_input_cost) < 0.000001 and abs(output_cost - expected_output_cost) < 0.000001:
        print("âœ… Cost precedence PASSED")
    else:
        print(f"âŒ Cost precedence FAILED: got ${input_cost:.6f}/${output_cost:.6f}, expected ${expected_input_cost:.6f}/${expected_output_cost:.6f}")
        success = False
    
    return success

def test_openinference_only():
    """Test extraction with only OpenInference attributes"""
    
    print("\nðŸ§ª Testing OpenInference-only extraction...")
    
    attributes = {
        "llm.model_name": "gpt-4o-mini",
        "llm.token_count.prompt": 80,
        "llm.token_count.completion": 40,
        "cost_prompt": 0.00008,
        "cost_completion": 0.00012,
        "openinference.span.kind": "LLM"
    }
    
    # Extract using our fixed logic
    input_tokens = attributes.get("gen_ai.usage.input_tokens", attributes.get("llm.token_count.prompt", 0))
    output_tokens = attributes.get("gen_ai.usage.output_tokens", attributes.get("llm.token_count.completion", 0))
    input_cost = attributes.get("gen_ai.usage.input_cost", attributes.get("cost_prompt", 0.0))
    output_cost = attributes.get("gen_ai.usage.output_cost", attributes.get("cost_completion", 0.0))
    
    print(f"Extracted: tokens={input_tokens}+{output_tokens}, cost=${input_cost:.6f}+${output_cost:.6f}")
    
    # Should use OpenInference values since GenAI not present
    expected_input_tokens = 80
    expected_output_tokens = 40
    expected_input_cost = 0.00008
    expected_output_cost = 0.00012
    
    success = True
    
    if input_tokens == expected_input_tokens and output_tokens == expected_output_tokens:
        print("âœ… OpenInference token extraction PASSED")
    else:
        print(f"âŒ OpenInference token extraction FAILED: got {input_tokens}/{output_tokens}, expected {expected_input_tokens}/{expected_output_tokens}")
        success = False
        
    if abs(input_cost - expected_input_cost) < 0.000001 and abs(output_cost - expected_output_cost) < 0.000001:
        print("âœ… OpenInference cost extraction PASSED")
    else:
        print(f"âŒ OpenInference cost extraction FAILED: got ${input_cost:.6f}/${output_cost:.6f}, expected ${expected_input_cost:.6f}/${expected_output_cost:.6f}")
        success = False
    
    return success

def main():
    """Run all tests"""
    print("ðŸš€ Testing Analytics Trace Data Extraction Fix")
    print("=" * 60)
    
    results = []
    
    # Test 1: Basic extraction
    results.append(test_extraction_logic())
    
    # Test 2: Fallback priority  
    results.append(test_fallback_priority())
    
    # Test 3: OpenInference only
    results.append(test_openinference_only())
    
    # Summary
    print("\n" + "=" * 60)
    print("ðŸ“Š TEST SUMMARY")
    print("=" * 60)
    
    all_passed = all(results)
    passed_count = sum(results)
    total_count = len(results)
    
    print(f"Tests passed: {passed_count}/{total_count}")
    
    if all_passed:
        print("ðŸŽ‰ ALL TESTS PASSED!")
        print("\nðŸ’¡ The trace extraction logic is working correctly.")
        print("   Cost, tokens, and spans should now appear in the Analytics tab")
        print("   when you run workflows in the UI.")
    else:
        print("âŒ SOME TESTS FAILED!")
        print("   Check the specific test failures above.")
    
    return all_passed

if __name__ == "__main__":
    import sys
    success = main()
    sys.exit(0 if success else 1)